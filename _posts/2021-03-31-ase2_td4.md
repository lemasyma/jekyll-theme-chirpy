---
title:          "ASE2: TD 4"
date:           2021-03-31 10:00
categories:     [tronc commun S8, ASE2]
tags:           [tronc commun, ASE2, S8, loi, binomial, estimateur, Fisher, FDCR, maximum de vraisemblance]
math: true
description: TD 4
---
Lien de la [note Hackmd](https://hackmd.io/@lemasymasa/S1ZS09br_)

# Exercice 16
Soit une variable aleatoire $X$ de loi de Poisson de parametre $\lambda$ ($\lambda\gt0$). Le but de cet exercice est de trouver une estimation de $\theta=e^{-\lambda}$.
$(X_1,X_2,...,X_n)$ un echantillon de $X$ et $Y_1,Y_2,...,Y_n$ des v.a. definies par $Y_i=1$ si $X_i=0$ et $Y_i=0$ sinon, $\forall i \in[[1,n]]$.

$$
S_n=\sum_{i=1}^nX_i\\
\bar Y_n=\frac{1}{n}\sum_{i=1}^nY_i\\
T_n=\biggr(\frac{n-1}{n}\biggr)^{S_n}
$$

1. Montrer que $\bar Y_n$ est un estimateur sans biais et convergent de $\theta=e^{-\lambda}$
2. Montrer que $T_n$ est un estimateur sans biais et convergent de $\theta$
3. a) Etuder le sens de variation de $f$ definie sur $\mathbb R_+$ par $f(t)=ne^{\frac{t}{n}}-e^{t}-n+1$ et deduire son signe
    b) En deduire que $T_n$ est un estimateur de $\bar Y_n$
  
<details markdown="1">
<summary>Solution</summary>

1.

$Y_i$ suit la loi de Bernoulli, le parametre de $Y_i$ est $P(Y_i=1)=P(X_i=0)=e^{-\lambda}=\theta$ donc $Y_i\sim\mathcal B(0)$

$$
E(\bar Y_n)=\frac{1}{n}\sum_{i=1}^nE(Y_i)=\frac{1}{n}\sum_{i=1}^n\theta=\frac{n\theta}{n}=\theta
$$

$\bar Y_n$ est sans biais.

$$
V(\bar Y_n)=\frac{1}{n^2}\sum_{i=1}^nV(Y_i)=\frac{n\theta(1-\theta)}{n^2}=\frac{\theta(1-\theta)}{n}\to_{n\to+\infty}0\\
V(\bar Y_n)\to_{n\to+\infty}0
$$

En appliquant Tchebychev: $\forall\varepsilon\gt0$

$$
P(\vert\bar Y_n-E(\bar Y_n)\vert\ge\varepsilon)\le\frac{V(\bar Y_n)}{\varepsilon^2}=\frac{\theta(1-\theta)}{n\varepsilon^2}\to_{n\to+\infty}0
$$

<div class="alert alert-success" role="alert" markdown="1">
Donc $\bar Y_n\to_{n\to+\infty}^P\theta$
</div>

2.

$$
T_n=\biggr(\frac{n-1}{n}\biggr)^{S_n}\\
\begin{aligned}
E(T_n) &= E\biggr(\biggr(\frac{n-1}{n}\biggr)^{S_n}\biggr)\\
&=\sum_{k=0}^{+\infty}\biggr(\frac{n-1}{n}\biggr)^kP(S_n=k) \text{ (car } E(\phi(X))=\sum_k\phi(k)P(X=k))\\
&=\sum_{k=0}^{+\infty}\biggr(\frac{n-1}{n}\biggr)^ke^{-n\lambda}\frac{(n\lambda)^k}{k!} \text{ (car } S_n=\sum_{i=1}^nX_i\text{ somme independantes de Poisson }\mathcal P(\lambda))
\end{aligned}
$$

Donc $S_n\sim\mathcal P(n\lambda)$

$$
\begin{aligned}
E(T_n)&=e^{-n\lambda}\sum_{k=0}^{+\infty}\frac{((n-1)\lambda)^k}{k!}\\
&= e^{-n\lambda}e^{(n-1)\lambda}\\
&=e^{-\lambda}=\theta
\end{aligned}
$$

<div class="alert alert-warning" role="alert" markdown="1">

Rappel:

$$
\sum_{0}^{+\infty}\frac{x^k}{k!}=e^x
$$

</div>

$T_n$ est sans biais

$$
\begin{aligned}
E(T_n^2)&=E((\frac{n-1}{n})^{2S_n})\\
&= \sum_0^{+\infty}(\frac{n-1}{n})^{2k}P(S_n=k)\\
&= \sum_0^{+\infty}(\frac{n-1}{n})^{2k}e^{-n\lambda}\frac{(n\lambda)^k}{k!}\\
&= e^{-n\lambda}\sum_{k=0}^{+\infty}\frac{(\frac{(n-1)^2\lambda}{n})^k}{k!}\\
&= e^{-n\lambda}e^{(n-1)^2\frac{\lambda}{n}}\\
&= e^{-n\lambda}e^{(n^2-2n+1)\frac{\lambda}{n}}\\
&= e^{-2\lambda+\frac{\lambda}{n}}=\theta^2e^{\frac{\lambda}{n}}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">
Donc

$$
E(T_n^2)=\theta^2e^{\frac{\lambda}{n}}
$$

</div>

$$
\begin{aligned}
V(T_n)&=E(T_n^2)-E^2(T_n)\\
&= \theta^2e^{\frac{\lambda}{n}}-\theta^2\\
&=\theta^2(e^{\frac{\lambda}{n}}-1)
\end{aligned}\\
\lim_{n\to+\infty}V(T_n)=0
$$

En utilisant Tchebychev: $\forall\varepsilon\gt0$

$$
P(\vert T_n-\theta\vert\ge\varepsilon)\lt\frac{V(\bar Y_n)}{\varepsilon^2}\to_{n\to+\infty}0\\
\Rightarrow T_n\to_{n\to+\infty}^P\theta
$$

3.a)

$$
f(t)=ne^{\frac{t}{n}}-e^t-n+1\quad\forall t\in\mathbb R_+\\
f'(t)=e^{\frac{t}{n}}-e^t\\
\begin{cases}
\forall n\ge1\quad \frac{t}{n}\le t\Rightarrow e^{\frac{t}{n}}\le e^t\Rightarrow f'(t)\le0 &\forall t\in\mathbb R_+\\
\forall t\gt0\quad f(t)\text{ est decroissante et comme } f(0)=0
&\begin{aligned}
&\Rightarrow \forall t\ge0, f(t)\le f(0)=0\\
&\Rightarrow\color{green}{f(t)\le0\quad\forall t\ge0}
\end{aligned}
\end{cases}
$$

b) 

$$
E(T_n)=\theta\\
E(\bar Y_n)=\theta
$$

Les deux estimateurs sont sans biais.

Comparons leurs variances

$$
V(\bar Y_n)=\frac{\theta(1-\theta)}{n}, V(T_n)=\theta^2(e^{\frac{\lambda}{n}}-1)\\
\begin{aligned}
V(T_n)-V(\bar Y_n)&=\theta^2(e^{\frac{\lambda}{n}}-1)-\frac{\theta(1-\theta)}{n}\\
&= \frac{\theta^2}{n}(ne^{\frac{\lambda}{n}}-n-\frac{1}{\theta}+1)\\
&= \frac{\theta^2}{n}(ne^{\frac{\lambda}{n}}-n-e^{\lambda}+1)\\
&= \frac{\theta^2}{n}f(\lambda)\\
\text{Or } f \text{ est negative}&\Rightarrow V(T_n)-V(\bar Y_n)\le 0\\
&\Rightarrow\color{green}{V(T_n)\le V(\bar Y_n)}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">
$T_n$ est un meilleur estimateur que $\bar Y_n$
</div>

</details>

# Exercice 17

Soit $X$ une v.a. de loi $\mathcal B(n,p)$ ou $p$ est inconnu..
On veut estimer le parametre $p$.
On considere un echantillon de $X$: $(X_1,X_2,...,X_n)$.

1. Determiner la vraisemblance de l'echantillon
2. Determiner l'estimateur de maximum de vraisemblance de $p$
3. Cet estimateur est-il sans biais ?
4. Est-il convergent ?
5. Montrer que cet estimateur est efficace

<details markdown="1">
<summary>Solution</summary>

$X\sim\mathcal B(N,p)$, $\theta=p$ inconnu.

1.

$$
L(x_1,x_2,...,x_n,p)=\frac{(N!)^n}{\Pi_{i=1}^nx_i!(N-x_i)!}p^{\sum_{i=1}^nx_i}(1-p)^{nN-\sum_{i=1}^nx_i}
$$

d'apres l'exercice 14.

2.

L'equation de la vraisemblance:

$$
\frac{\delta \ln L}{\delta p}=0\\
\ln L(x_1,...,x_n,p)=\ln\biggr(\frac{(N!)^n}{\Pi_{i=1}^nx_i!(N-x_i)!}\biggr)+\sum_{i=1}^nx_i\ln(p)+(nN-\sum_{i=1}^nx_i)\ln(1-p)\\
\begin{aligned}
\frac{\delta \ln L}{\delta p}&=\frac{1}{p}\sum_{i=1}^nx_i+(nN-\sum_{i=1}^nx_i)\frac{-1}{1-p}=0\\
&\Leftrightarrow (1-p)\sum_{i=1}^nx_i-p(nN-\sum_{i=1}^nx_i)=0\\
&\Leftrightarrow \sum_{i=1}^nx_i-pnN=0\\
&\Leftrightarrow \color{green}{\hat p=\frac{1}{nN}\sum_{i=1}^nx_i} \text{ estimation ponctuelle de }p
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">
L'estimateur de maximum de vraisemblance est

$$
T_n=\frac{1}{nN}\sum_{i=1}^nX_i
$$

</div>

3.

*$Tn$ sans biais ?*

$$
\begin{aligned}
E(T_n)&=\frac{1}{nN}\sum_{i=1}^nE(X_i)\\
&=\frac{1}{nN}\sum_{i=1}^nNp\\
&= \frac{nNp}{nN}=\color{green}{p}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">

$$
E(T_n) = p
$$

</div>

$T_n$ est sans biais.

4.

*Convergence ?*


<div class="alert alert-warning" role="alert" markdown="1">

Rappel:

$$
V(aX) = a^2\times V(X)
$$

</div>

$$
\begin{aligned}
V(T_n)&=\frac{1}{n^2N^2}\sum_{i=1}^nV(X_i)\\
&=\frac{1}{n^2N^2}\sum_{i=1}^nNp(1-p)\\
&= \frac{nNp(1-p)}{n^2N^2}\\
&=\frac{p(1-p)}{nN}\to_{n\to+\infty}0
\end{aligned}
$$

D'apres Tchebychev $\forall\varepsilon\gt0$

$$
P(\vert T_n-E(T_n)\vert\ge\varepsilon)\le\frac{V(T_n)}{\varepsilon^2}\\
\Rightarrow P(\vert T_n-p\vert\ge\varepsilon)\le\frac{p(1-p)}{nN\varepsilon^2}\to_{n\to+\infty}0
$$

<div class="alert alert-success" role="alert" markdown="1">
Donc

$$
T_n\to_{n\to+\infty}^Pp
$$

</div>

$T_n$ converge en probabilite vers $p$.

5.

*Efficacite*

$$
\underbrace{I_n(p)}_{\text{information de Fisher}}=-E(\frac{\delta^2\ln L}{\delta p^2})\\
\frac{\delta\ln L}{\delta p}=\frac{1}{p}\sum_{i=1}^nx_i-(nN-\sum_{i=1}^nx_i)\frac{1}{1-p}\\
\frac{\delta^2\ln L}{\delta p^2}=-\frac{1}{p^2}\sum_{i=1}^nx_i+(nN-\sum_{i=1}^nx_i)\frac{-1}{(1-p)^2}\\
\begin{aligned}
E(\frac{\delta\ln L}{\delta p^2}) &=-\frac{1}{p^2}\sum_{i=1}^nE(x_i)+(nN-\sum_{i=1}^nE(x_i))\frac{-1}{(1-p)^2}\\
&=-\frac{1}{p^2}nNp+\frac{1}{(1-p)^2}(-nN+nNp)\\
&= \frac{-nN}{p}+\frac{(-nN)}{1-p}\\
&= \frac{-nN(1-p)-nNp}{p(1-p)}\\
&= \frac{-nN}{p(1-p)}
\end{aligned}\\
I_n(p) = -E(\frac{\delta\ln L}{\delta p^2})=\frac{nN}{p(1-p)}
$$

<div class="alert alert-success" role="alert" markdown="1">
Donc

$$
I_n(p)=\frac{nN}{p(1-p)}
$$
</div>

information de Fisher.

Or:

$$
V(T_n)=\frac{p(1-p)}{nN}\Rightarrow \color{green}{V(T_n)=\frac{1}{I_n(p)}}
$$

Conclusion: $T_n$ est efficace

</details>

# Exercice 18
Soit $X$ une distribution de Poisson $\mathcal P(\theta)$ ou $\theta$ inconnu.
$(X_1,...,X_n)$ un echantillon de $X$.
1. Determiner la vraisemblance
2. Determiner un estimateur de $\theta$
3. Est-il sans biais ? Convergent ?
4. Est-il efficace ?

<details markdown="1">
<summary>Solution</summary>

$X\sim\mathcal P(\theta)$ Poisson de parametre $\theta$, $\theta$: inconnu.

1.La vraisemblance est:

$$
\begin{aligned}
L(x_1,x_2,...,x_n)&=\Pi_{i=1}^nP(X_i=x_i)\\
&=\frac{e^{-n\theta}\theta^{\sum_{i=1}^nx_i}}{\Pi_{i=1}^nx_i!} \text{cf. exercice 14.}
\end{aligned}
$$


2.Methode du maximum de vraisemblance

$$
\frac{\delta\ln L}{\delta\theta}=0 \text{ (eq. de la vraisemblance)}\\
\ln L(x_1,...,x_m,\theta)=-n\theta+\sum_{i=1}^n\ln \theta-\ln(\Pi_{i=1}^nx_i!)\\
\begin{aligned}
\frac{\delta\ln L}{\delta\theta}=0&\Leftrightarrow -n+\frac{1}{\theta}\sum_{i=1}^nx_i=0\\
&\Leftrightarrow\hat \theta=\frac{1}{n}\sum_{i=1}^nx_i\text{ estimation ponctulle de }\theta
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">
L'estimateur de $\theta$ est $T_n=\frac{1}{n}\sum_{i=1}^nX_i$
</div>

3.*$Tn$ sans biais ?*

$$
\begin{aligned}
E(T_n)&=\frac{1}{n}\sum_{i=1}^nE(X_i)\\
&=\frac{1}{n}\sum_{i=1}^n\theta\\
&= \frac{n\theta}{n}=\color{green}{\theta}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">

$$
E(T_n)=\theta
$$

$T_n$ est sans biais.

</div>

*Convergence?*

$$
\begin{aligned}
V(T_n)&=\frac{1}{n^2}\sum_{i=1}^nV(X_i)\\
&=\frac{n\theta}{n}\\
&=\frac{\theta}{n}
\end{aligned}\\
V(T_n)=\frac{\theta}{n}\to_{n\to+\infty}0
$$

Donc en utilisant Tchebychev: $\forall\varepsilon\gt0$

$$
P(\vert T_n-E(T_n)\vert\ge\varepsilon)\le\frac{V(T_n)}{\varepsilon^2}\\
\Rightarrow P(\vert T_n-\theta\vert\ge\varepsilon)\le\frac{\theta}{n\varepsilon^2}\to_{n\to+\infty}0\\
$$

<div class="alert alert-success" role="alert" markdown="1">
Donc 

$$
T_n\to_{n\to+\infty}^P\theta
$$

</div>

4.*Efficacite*

On calcule l'information de Fisher:

$$
I_n(\theta)=-t(\frac{\delta^2\ln L}{\delta \theta^2})\\
\frac{\delta \ln L}{\delta\theta}=-n+\frac{1}{\theta}\sum_{i=1}^nx_i\\
\begin{aligned}
I_n(\theta)&=-E(\frac{\delta^2\ln L}{\delta\theta^2})\\
&=\frac{1}{\theta^2}\sum_{i=1}^nE(x_i)\\
&=\frac{n\theta}{\theta^2}=\color{green}{\frac{n}{\theta}}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">

$$
V(T_n)=\frac{\theta}{n}=\frac{1}{I_n(\theta)}
$$

$T_n$ est efficace.

</div>

</details>

# Exercice 19
Soit $X$ une v.a. continue de densite

$$
f(x)=\frac{A}{x^{1+\frac{1}{\theta}}}\quad \forall x\ge 1, \theta\gt0 
$$

1. Determiner $A$ en fonction de $\theta$
2. Soit $(X_1,X_2,...,X_n)$ un echantillon de $X$, Determiner la vraisemblance de cet echantillon
3. Determiner l'estimateur du maximum de vraisemblance de $\theta$
4. Est-il sans biais ? Convergent ?
5. Est-il efficace ?

<details markdown="1">
<summary>Solution</summary>

1.$f$ etant une densite: $\int_{\mathbb R}f(x)dx=1$

$$
A\int_{1}^{+\infty}\frac{1}{x^{1+\frac{1}{\theta}}}dx=1\\
A\biggr[\frac{-\theta}{x^{\frac{1}{\theta}}}\biggr]\\
\Rightarrow A\theta=1\Rightarrow\color{green}{A=\frac{1}{\theta}}
$$

2.

$$
\begin{aligned}
L(x_1,x_2,...,x_n,\theta)&=\Pi_{i=1}^nf(x_i)\\
&=\Pi_{i=1}^n\frac{1}{\theta}\frac{1}{x_i^{1+\frac{1}{\theta}}}\\
&=\frac{1}{\theta^n}\frac{1}{\Pi_{i=1}^nx_i^{1+\frac{1}{\theta}}}
\end{aligned}
$$

3.

$$
\ln L(x_1,x_2,...,x_n,\theta)=-n\ln\theta-(1+\frac{1}{\theta})\sum_{i=1}^n\ln x_i
$$

<div class="alert alert-warning" role="alert" markdown="1">
Equation de la vraisemblance:

$$
\frac{\delta\ln L}{\delta\theta}=0
$$

</div>

$$
\begin{aligned}
\frac{\delta\ln L}{\delta\theta}&=\frac{-n}{\theta}+\frac{1}{\theta^2}\sum_{i=1}^n\ln x_i=0\\
&\Rightarrow\color{green}{\hat\theta=\frac{1}{n}\sum_{i=1}^n\ln x_i}
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">

L'estimateur de vraisemblance:

$$
T_n=\frac{1}{n}\sum_{i=1}^n\ln x_i
$$

</div>

4.

$$
\begin{aligned}
E(T_n)&=\frac{1}{n}\sum_{i=1}^nE(\ln x_i)\\
&=\frac{nE(\ln x)}{n}\\
&=E(\ln x)
\end{aligned}
$$

or:

$$
\begin{aligned}
E(\ln x)&=\int_{1}^{+\infty}\ln xf(x)dx\\
&=\int_{1}^{+\infty}\frac{\ln x}{\theta ^{1+\frac{1}{\theta}}}dx
\end{aligned}
$$

On integre par parties:

$$
\begin{cases}
v=\ln x &v'=\frac{1}{x}\\
u'=\frac{1}{\theta}x^{-1-\frac{1}{\theta}} &u=-x^{-\frac{1}{\theta}}
\end{cases}\\
\begin{aligned}
E(\ln x)&=\underbrace{[-x^{-\frac{1}{\theta}}\ln x]_1^{+\infty}}_{=0 \text{ quand }x\to+\infty}+\int_1^{+\infty}x^{-1-\frac{1}{\theta}}dx\\
&=[-\theta x^{-\frac{1}{\theta}}]_1^{+\infty}=\theta
\end{aligned}
$$

<div class="alert alert-success" role="alert" markdown="1">

Donc $E(T_n)=\theta$ sans biais.

</div>

*Convergence ?*

$$
\begin{aligned}
V(T_n)&=\frac{1}{n^2}\sum_{i=1}^nV(\ln x_i)\\
&= \frac{nV(\ln X)}{n^2}\\
&=\frac{V(\ln X)}{n}
\end{aligned}\\
\begin{aligned}
E(\ln^2x)&=\int_1^{+\infty}\frac{\ln^2x}{\theta x^{1+\frac{1}{\theta}}}dx\\
&=\underbrace{[-x^{-\frac{1}{\theta}}\ln^2x]_1^{+\infty}}_{=0 \text{ quand }x\to+\infty}+\int_1^{+\infty}\frac{1\ln x}{x^{1+\frac{1}{\theta}}}dx
\end{aligned}\\
\begin{cases}
v=\ln^2x &v'=2(\ln x)\frac{1}{x}\\
u'=\frac{1}{\theta}x^{-1-\frac{1}{\theta}}, &u=-x^{-\frac{1}{\theta}}
\end{cases}\\
\begin{aligned}
E(\ln^2x)&=1\theta\int_1^{+\infty}\frac{\ln xdx}{\theta x^{1+\frac{1}{\theta}}}\\
&=2\theta E(\ln x)\\
&=2\theta^2
\end{aligned}\\
\begin{aligned}
V(T_n)&=\frac{E(\ln^2x)-E^2(\ln x)}{n}\\
&=\frac{1}{n}(2\theta^2-\theta^2)\\
&=\frac{\theta^2}{n}
\end{aligned}\\
V(T_n)=\frac{\theta^2}{n}\to_{n\to+\infty}0
$$

<div class="alert alert-success" role="alert" markdown="1">
D'apres Tchebychev $T_n\to_{n\to+\infty}^P\theta$
</div>

5.*Efficacite*

$$
I_n(\theta)=-E(\frac{\delta^2\ln L}{\delta\theta^2})\\
\frac{\delta\ln L}{\delta\theta}=-\frac{n}{\theta}+\frac{1}{\theta^2}\sum_{i=1}^n\ln x-i\\
\frac{\delta^2\ln L}{\delta\theta^2}=\frac{n}{\theta^2}-\frac{2}{\theta^3}\sum_{i=1}^n\ln x_i\\
\begin{aligned}
I_n(\theta)&=-E(\frac{\delta^2\ln L}{\delta\theta^2})\\
&=-\frac{n}{\theta}+\frac{2}{\theta^3}\sum_{i=1}^nE(\ln x_i)\\
&=-\frac{n}{\theta^2}+\frac{2}{\theta^3}nE(\ln x)\\
&=-\frac{n}{\theta^2}+\frac{2}{\theta^3}n\theta=\color{green}{\frac{n}{\theta^2}}
\end{aligned}
$$

Or $V(T_n)=\frac{\theta^2}{n}=\frac{1}{I_n(\theta)}$

<div class="alert alert-success" role="alert" markdown="1">
Donc $T_n$ est efficace. 
</div>

</details>
